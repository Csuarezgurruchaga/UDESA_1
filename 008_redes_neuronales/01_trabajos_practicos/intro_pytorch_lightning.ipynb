{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"indice\"></a> \n",
    "## Índice de la notebook\n",
    "\n",
    "[1) PYTORCH AGAINS PYTORCH LIGHTNING](#1)<BR>\n",
    "\n",
    "[2) TURNING A PYTORCH CODE FROM intro_pytorch to PYTORCH LIGHTNING](#2)<BR>\n",
    "\n",
    "[3) INTRODUCTION TO PYTORCH LIGHTNING](#3)\n",
    "-    [3.1)ADDING VALIDATE AND TEST STEPS](#4)<BR>\n",
    "-    [3.2)SAVING AND LOADING CHECKPOINTS](#5)<BR>\n",
    "-    [3.3)EARLY STOPPING (REGULARIZATION)](#6)<BR>\n",
    "-    [3.4)DEBUG YOUR MODEL](#7)<BR>\n",
    "-    [3.5)TRACK AND VISUALIZATION](#8)<BR>\n",
    "-    [3.6)DEPLOY MODELS INTO PRODUCCTION](#9)<BR>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1)PYTORCH AGAINS PYTORCH LIGHTNING \n",
    "\n",
    "[Ir a índice](#indice)\n",
    "\n",
    "#### Some details that we dont need to worry about with pyTorch Lightening\n",
    "\n",
    "## <u>setting</u>\n",
    "model.eval()\n",
    "model.train()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "data.to(device)\n",
    "#### -> easy GPU/TPU support\n",
    "#### -> scale GPUs\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    ...\n",
    "\n",
    "x = x.detach()\n",
    "\n",
    "#### Bonus : - Tensorboard support\n",
    "####         - prints tips/hints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    "\n",
    "### 2) TURNING A PYTORCH CODE FROM intro_pytorch to PYTORCH LIGHTNING\n",
    "\n",
    "[Ir a índice](#indice)\n",
    "\n",
    "*Here i transform from PyTorch to PyTorch Lightning a code which aims to predict the labels of handwritten digits from MNIST dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class LightNeuralNet(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LightNeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X_train, y_train = batch\n",
    "        X_train = X_train.reshape(-1, 28*28)\n",
    "        \n",
    "        # forward pass\n",
    "        y_hat = self(X_train)\n",
    "        loss = F.cross_entropy(y_hat, y_train)\n",
    "        \n",
    "        # Shows in Tensorboard\n",
    "        self.log('train_loss', loss, on_epoch=False)\n",
    "        return loss\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = torchvision.datasets.MNIST(root = './intro_pytorch_data', \n",
    "                                                   train = True, download = False, \n",
    "                                                   transform = transforms.ToTensor())\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, \n",
    "                                                   num_workers = 8, shuffle = True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = torchvision.datasets.MNIST(root = './intro_pytorch_data', \n",
    "                                                 train = False, download = False, \n",
    "                                                 transform = transforms.ToTensor())\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 32, num_workers = 8, shuffle = False)\n",
    "        return val_loader\n",
    "    \n",
    "    \n",
    "    ################# VALIDATION STEP #################\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X_val, y_val = batch\n",
    "        X_val = X_val.reshape(-1, 28*28)\n",
    "        \n",
    "        # forward pass\n",
    "        y_hat = self(X_val)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y_val)\n",
    "        self.log('avg_val_loss', loss)#pytorch Lightning automatically mean accumulates the metric and averages by epoch\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model = LightNeuralNet(input_size, hidden_size, num_classes)\n",
    "    trainer = Trainer(max_epochs = num_epochs, # gpus=1 is the number of gpu used//tpu_cores is the number of cores\n",
    "                      fast_dev_run = True, deterministic=True) # auto_lr_rate = True, find the best lr\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open Tensorboard: <br>\n",
    "`tensorboard --logdir=lightning_logs`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    "### 3) INTRODUCTION TO PYTORCH LIGHTNING<br>\n",
    "\n",
    "\n",
    "[Ir a índice](#indice)\n",
    "##### Here i run the introductory guide from PyTorch Lightning website implementing an Vanilla Autoencoder with pytorch<br> \n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/levels/core_skills.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/csuarezgurruchaga/opt/anaconda3/envs/redes/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 50.4 K\n",
      "1 | decoder | Decoder | 51.2 K\n",
      "------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 1139/60000 [00:11<10:02, 97.68it/s, loss=0.0502, v_num=2]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=3)\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features= 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=28*28)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "# Defining the lightling module\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder\n",
    "        self.decoder = Decoder\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training loop\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)        \n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(X_hat, X)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "# Defining the training dataset\n",
    "train_dataset = MNIST(root = './intro_pytorch_data', train = True, download = False, \n",
    "                        transform = transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle= True, num_workers=8)\n",
    "        \n",
    "# instance the model\n",
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "\n",
    "# train the model using the lightining trainer\n",
    "trainer = pl.Trainer()\n",
    "trainer.fit(model= autoencoder, train_dataloaders= train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    "### 3.1)ADDING VALIDATE AND TEST STEPS<br>\n",
    "\n",
    "\n",
    "[Ir a índice](#indice)\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/common/evaluation_basic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "/Users/csuarezgurruchaga/opt/anaconda3/envs/redes/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /Users/csuarezgurruchaga/Desktop/UDESA/008_redes_neuronales/01_trabajos_practicos/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 50.4 K\n",
      "1 | decoder | Decoder | 51.2 K\n",
      "------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "/Users/csuarezgurruchaga/opt/anaconda3/envs/redes/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1600: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it, loss=0.142, v_num=]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:06<00:00,  3.23s/it, loss=0.142, v_num=]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 122.53it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.09412974864244461\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.09412974864244461}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=3)\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features= 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=28*28)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "# Defining the lightling module\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder\n",
    "        self.decoder = Decoder\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training loop\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)        \n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(X_hat, X)\n",
    "        return loss\n",
    "    \n",
    "    #Define the validation loop\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)\n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        val_loss = F.mse_loss(X_hat, X)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        return val_loss\n",
    "        \n",
    "    #Define the test loop\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)\n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        test_loss = F.mse_loss(X_hat, X)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "# Defining the training dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "#### ADDING VALIDATION LOOP ####\n",
    "train_dataset = MNIST(root = './intro_pytorch_data', train = True, download = False, \n",
    "                        transform = transform)\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_dataset) * 0.8)\n",
    "valid_set_size = len(train_dataset) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle= True, num_workers=8)\n",
    "val_loader = DataLoader(dataset=valid_dataset, shuffle= False, num_workers=8)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "test_dataset = MNIST(root = './intro_pytorch_data', train = False, download = False, \n",
    "                        transform = transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle= False, num_workers=8)\n",
    "# instance the model\n",
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "\n",
    "# train the model using the lightining trainer FOR TRAIN/VAL AND TEST DATASETS\n",
    "trainer = pl.Trainer(max_epochs = 2, fast_dev_run = True)\n",
    "trainer.fit(model= autoencoder, train_dataloaders= train_loader, \n",
    "                                val_dataloaders=val_loader)\n",
    "\n",
    "\n",
    "#### Train with the test loop ####\n",
    "trainer.test(model = autoencoder, dataloaders=test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    "### 3.2)SAVING AND LOADING CHECKPOINTS<br>\n",
    "[Ir a índice](#indice)\n",
    "\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing_basic.html#save-a-checkpoint\n",
    "\n",
    "- simply by using the Trainer you get automatic checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: checkpoints/lightning_logs\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 50.4 K\n",
      "1 | decoder | Decoder | 51.2 K\n",
      "------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 60000/60000 [06:10<00:00, 162.12it/s, loss=0.0374, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 60000/60000 [06:10<00:00, 162.12it/s, loss=0.0374, v_num=0]\n",
      "Testing DataLoader 0: 100%|██████████| 10000/10000 [00:23<00:00, 422.96it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.04018773138523102\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.04018773138523102}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=3)\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features= 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=28*28)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "# Defining the lightling module\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder\n",
    "        self.decoder = Decoder\n",
    "        self.save_hyperparameters() # Saving the hiperparameters pased to the init\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training loop\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)        \n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(X_hat, X)\n",
    "        return loss\n",
    "    \n",
    "    #Define the validation loop\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)\n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        val_loss = F.mse_loss(X_hat, X)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        return val_loss\n",
    "        \n",
    "    #Define the test loop\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)\n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        test_loss = F.mse_loss(X_hat, X)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "# Defining the training dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "#### ADDING VALIDATION LOOP ####\n",
    "train_dataset = MNIST(root = './intro_pytorch_data', train = True, download = False, \n",
    "                        transform = transform)\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_dataset) * 0.8)\n",
    "valid_set_size = len(train_dataset) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle= True, num_workers=8)\n",
    "val_loader = DataLoader(dataset=valid_dataset, shuffle= False, num_workers=8)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "test_dataset = MNIST(root = './intro_pytorch_data', train = False, download = False, \n",
    "                        transform = transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle= False, num_workers=8)\n",
    "# instance the model\n",
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "\n",
    "# train the model using the lightining trainer FOR TRAIN/VAL AND TEST DATASETS\n",
    "# saves checkpoints to 'some/path/' at every epoch end\n",
    "trainer = pl.Trainer(max_epochs = 2, fast_dev_run = False, default_root_dir= \"./checkpoints\")\n",
    "trainer.fit(model= autoencoder, train_dataloaders= train_loader, \n",
    "                                val_dataloaders=val_loader)\n",
    "\n",
    "\n",
    "#### Train with the test loop ####\n",
    "trainer.test(model = autoencoder, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading a saved checkpoint ####\n",
    "model = pl.LightningModule.load_from_checkpoint(\"checkpoints/epoch=0-step=48000.ckpt\")\n",
    "print(model.learning_rate) # also if you set self.save_hyperparameters(), you can call for the hiperparameters saved.\n",
    "# disable randomness, dropout, etc...\n",
    "model.eval()\n",
    "\n",
    "# predict with the model\n",
    "y_hat = model(X)\n",
    "\n",
    "#### For resuming training stage ####\n",
    "model = LitAutoEncoder(Encoder, Decoder)\n",
    "trainer = Trainer()\n",
    "\n",
    "# automatically restores model, epoch, step, LR schedulers, etc...\n",
    "trainer.fit(model, ckpt_path=\"some/path/to/my_checkpoint.ckpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    "### 3.3) EARLY STOPPING (REGULARIZATION)<br>\n",
    "[Ir a índice](#indice)\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/common/early_stopping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 50.4 K\n",
      "1 | decoder | Decoder | 51.2 K\n",
      "------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 60000/60000 [06:08<00:00, 162.61it/s, loss=0.0458, v_num=19]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 60000/60000 [06:09<00:00, 162.60it/s, loss=0.0458, v_num=19]\n",
      "Testing DataLoader 0: 100%|██████████| 10000/10000 [00:22<00:00, 438.18it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.04136969521641731\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.04136969521641731}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=3)\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features= 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=28*28)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "# Defining the lightling module\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder\n",
    "        self.decoder = Decoder\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training loop\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)        \n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(X_hat, X)\n",
    "        return loss\n",
    "    \n",
    "    #Define the validation loop\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)\n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        val_loss = F.mse_loss(X_hat, X)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        return val_loss\n",
    "        \n",
    "    #Define the test loop\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)\n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        test_loss = F.mse_loss(X_hat, X)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "# Defining the training dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "#### ADDING VALIDATION LOOP ####\n",
    "train_dataset = MNIST(root = './intro_pytorch_data', train = True, download = False, \n",
    "                        transform = transform)\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_dataset) * 0.8)\n",
    "valid_set_size = len(train_dataset) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle= True, num_workers=8)\n",
    "val_loader = DataLoader(dataset=valid_dataset, shuffle= False, num_workers=8)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "test_dataset = MNIST(root = './intro_pytorch_data', train = False, download = False, \n",
    "                        transform = transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle= False, num_workers=8)\n",
    "# instance the model\n",
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "\n",
    "# train the model using the lightining trainer FOR TRAIN/VAL AND TEST DATASETS\n",
    "trainer = pl.Trainer(max_epochs = 2, callbacks=[EarlyStopping(monitor=\"val_loss\", mode = 'min')])\n",
    "\n",
    "trainer.fit(model= autoencoder, train_dataloaders= train_loader, \n",
    "                                val_dataloaders=val_loader)\n",
    "\n",
    "\n",
    "#### Train with the test loop ####\n",
    "trainer.test(model = autoencoder, dataloaders=test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    "### 3.4) DEBUG YOUR MODEL<br>\n",
    "[Ir a índice](#indice)\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/debug/debugging_basic.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    "### 3.5) TRACK AND VISUALIZATION<br>\n",
    "[Ir a índice](#indice)\n",
    "\n",
    "In model development, we track values of interest such as the validation_loss to visualize the learning process for our models. Model development is like driving a car without windows, charts and logs provide the windows to know where to drive the car.<BR>\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/visualize/logging_basic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 50.4 K\n",
      "1 | decoder | Decoder | 51.2 K\n",
      "------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 60000/60000 [05:40<00:00, 176.06it/s, loss=0.0416, v_num=21]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 60000/60000 [05:40<00:00, 176.06it/s, loss=0.0416, v_num=21]\n",
      "Testing DataLoader 0: 100%|██████████| 10000/10000 [00:23<00:00, 421.62it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss          0.040752630680799484\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.040752630680799484}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=3)\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features= 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=28*28)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "# Defining the lightling module\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, Encoder, Decoder, batch_size): # we set the \"batch_size\" parameter, to use auto_scale_batch_size\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder\n",
    "        self.decoder = Decoder\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training loop\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)        \n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(X_hat, X)\n",
    "        return loss\n",
    "    \n",
    "    #Define the validation loop\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)\n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        val_loss = F.mse_loss(X_hat, X)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        return val_loss\n",
    "        \n",
    "    #Define the test loop\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = X.view(X.size(0), -1)\n",
    "        z = self.encoder(X)\n",
    "        X_hat = self.decoder(z)\n",
    "        test_loss = F.mse_loss(X_hat, X)\n",
    "        self.log(\"test_loss\", test_loss, prog_bar = True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "# Defining the training dataset\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "#### ADDING VALIDATION LOOP ####\n",
    "train_dataset = MNIST(root = './intro_pytorch_data', train = True, download = False, \n",
    "                        transform = transform)\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_dataset) * 0.8)\n",
    "valid_set_size = len(train_dataset) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_dataset, valid_dataset = random_split(train_dataset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle= True, num_workers=8)\n",
    "val_loader = DataLoader(dataset=valid_dataset, shuffle= False, num_workers=8)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "test_dataset = MNIST(root = './intro_pytorch_data', train = False, download = False, \n",
    "                        transform = transform)\n",
    "test_loader = DataLoader(dataset=test_dataset, shuffle= False, num_workers=8)\n",
    "# instance the model\n",
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "\n",
    "# train the model using the lightining trainer FOR TRAIN/VAL AND TEST DATASETS\n",
    "trainer = pl.Trainer(max_epochs = 2, auto_scale_batch_size= True, auto_lr_find= True) # Finds the better batch size and lr when we run trainer.tune(model)\n",
    "\n",
    "# trainer.tune(model) # Runs routines to find the better hiperparameters before training, we also need set 'auto_lr_find' in trainer \n",
    "\n",
    "trainer.fit(model= autoencoder, train_dataloaders= train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "#### Train with the test loop ####\n",
    "trainer.test(model = autoencoder, dataloaders=test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> \n",
    "### 3.6)DEPLOY MODELS INTO PRODUCTION<BR>\n",
    "[Ir a índice](#indice)\n",
    "\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/deploy/production_basic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | l1   | Linear | 392 K \n",
      "1 | relu | ReLU   | 0     \n",
      "2 | l2   | Linear | 5.0 K \n",
      "--------------------------------\n",
      "397 K     Trainable params\n",
      "0         Non-trainable params\n",
      "397 K     Total params\n",
      "1.590     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:11<00:00,  5.88s/it, loss=2.32, v_num=]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:11<00:00,  5.88s/it, loss=2.32, v_num=]\n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 82.07it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([-0.1411, -0.1731,  0.2701,  0.5368,  0.0308,  0.2903,  0.1149, -0.0087,\n",
       "         -0.1700, -0.0348])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting a PyTorch code from intro_pytorch to PyTorch Lightening\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class LightNeuralNet(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LightNeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X_train, y_train = batch\n",
    "        X_train = X_train.reshape(-1, 28*28)\n",
    "        \n",
    "        # forward pass\n",
    "        y_hat = self(X_train)\n",
    "        loss = F.cross_entropy(y_hat, y_train)\n",
    "        \n",
    "        # Shows in Tensorboard\n",
    "        self.log('train_loss', loss, on_epoch=False)\n",
    "        return loss\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = torchvision.datasets.MNIST(root = './intro_pytorch_data', \n",
    "                                                   train = True, download = False, \n",
    "                                                   transform = transforms.ToTensor())\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, \n",
    "                                                   num_workers = 8, shuffle = True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = torchvision.datasets.MNIST(root = './intro_pytorch_data', \n",
    "                                                 train = False, download = False, \n",
    "                                                 transform = transforms.ToTensor())\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 32, num_workers = 8, shuffle = False)\n",
    "        return val_loader\n",
    "    \n",
    "    \n",
    "    ################# VALIDATION STEP #################\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X_val, y_val = batch\n",
    "        X_val = X_val.reshape(-1, 28*28)\n",
    "        \n",
    "        # forward pass\n",
    "        y_hat = self(X_val)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, y_val)\n",
    "        self.log('avg_val_loss', loss)#pytorch Lightning automatically mean accumulates the metric and averages by epoch\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        output = self(batch) \n",
    "        return output \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model = LightNeuralNet(input_size, hidden_size, num_classes)\n",
    "    trainer = Trainer(max_epochs = num_epochs, # gpus=1 is the number of gpu used//tpu_cores is the number of cores\n",
    "                      fast_dev_run = True, deterministic=True) # auto_lr_rate = True, find the best lr\n",
    "    trainer.fit(model)\n",
    "    \n",
    "    \n",
    "\n",
    "test_data = torch.rand(784,784)\n",
    "trainer.predict(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "466813897a52447f1831c184b0700fb7b7f042a70becf90568dd3beaa877b3ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
