{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNUnHhYXYlB0"
   },
   "source": [
    "# Aprendizaje Supervisado II\n",
    "## Taller 2: Árboles de decisión\n",
    "\n",
    "\n",
    "\n",
    "*   1. Ejercicios en papel\n",
    "*   2. Ejercicios para pensar\n",
    "*   3. Implementar un árbol de decisión\n",
    "*   4. Comparaciónes con Scikit-learn\n",
    "\n",
    "\n",
    "---\n",
    "Los datos y ejemplos del clima fueron tomados el libro: Mitchell, T.M. Machine Learning (McGraw-Hill, 1997)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zIUEdsYVU5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RfP3W25Vpjl"
   },
   "source": [
    "## 1. Ejercicios en papel\n",
    " 1.1 Los Arboles de decisión nos permiten modelar logica booleana y tablas de verdad. Pensando cada atributo como una variable proposicional.\n",
    "\n",
    " Arme diferentes árboles para cada formula booleana.\n",
    " \n",
    "  a. $\\neg p$\n",
    "\n",
    "  b. $p \\wedge q$\n",
    "\n",
    "  c. $p \\vee q$ \n",
    "\n",
    "  d. $p \\vee (q \\wedge r)$\n",
    "   \n",
    "  e. $p \\;\\textrm{XOR}\\; q$\n",
    "   \n",
    "  f. $(p \\wedge \\neg q) \\vee (r \\wedge s)$  \n",
    "\n",
    "  Por ejemplo el árbol para $p \\wedge q$ sería (cualquiera de las dos opciones es válida):\n",
    "\n",
    "  ![alt text](https://docs.google.com/drawings/d/e/2PACX-1vS3C8TORN-PC0FUGoWUcEWmhoD0sJuL3WINdQHMC0sP4AmqakQPA8xQsA-L9fyaLK1cHKB3trfkoBUF/pub?w=923&h=277)\n",
    "\n",
    "1.2. Hacer en papel y lápiz el árbol de decisión correspondiente a entrenar con los datos de Juego al tenis.\n",
    "\n",
    "![alt text](https://docs.google.com/drawings/d/e/2PACX-1vRVE4maM4CQKi8sM6xD4ZZqC5RPlQfyUDyXiHlMwvrvEArobReaqG-9gCUwjteT-uqy4kw5bgqY-5XV/pub?w=447&h=325)\n",
    "\n",
    "1.3. Armar 3 ejemplos posibles de entradas nuevas y usar el árbol del ejercicio anterior para predecir la clase de salida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br6-5KjlZExI"
   },
   "source": [
    "## 2. Ejercicios para pensar\n",
    "\n",
    "2.1. ¿Cuál el sesgo inductivo del ´arbol de decisión?\n",
    "\n",
    "En el aprendizaje automático, el sesgo inductivo de un algoritmo es el conjunto de suposiciones que el clasificador utiliza para predecir la salida dadas las entradas que aún no ha encontrado (Mitchell, 1980). El aprendizaje automático tiene como objetivo construir algoritmos que puedan aprender una determinada función objetiva. Para este propósito, el algoritmo de aprendizaje se proporciona con un conjunto de entrenamiento, que contiene ejemplos de la relación subyacente entre los valores de entrada y salida de la función objetivo. Por lo tanto, el clasificador debe aproximar la función objetivo de estos ejemplos. El tipo de suposiciones que el clasificador hace sobre la naturaleza de la función objetiva se llama sesgo inductivo (Mitchell, 1980; desJardins y Gordon, 1995). Un ejemplo clásico de sesgo inductivo es la navaja de Occam. Este principio supone que se debe preferir la hipótesis más simple consistente con el conjunto de entrenamiento.\n",
    "\n",
    "2.2. ¿Cuál es la motivación de usar entropía y ganancia de información?\n",
    "\n",
    "El concepto de entropia refiere al nivel de desorden de un conjunto, y la ganancia de informacion refiere a cuanto mejora esa variable mi cantidad de entropia respecto al nodo anterior TENIENDO EN CUENTA LA CANTIDAD DE OBSERVACIONES QUE TIENE EL NODO QUE ORDENO, como un peso. El nivel de entropia de 0.5 es el peor escenario.\n",
    "\n",
    "2.3. ¿Qué sucede cuando dos atributos empatan en ganancia de información? ¿Esta decisión es parte del sesgo inductivo?\n",
    "\n",
    "Se puede plantear un criterio de eleccion para esta situacion. En caso de CART, podria observarse la siguiente particion, cual feature del nodo anterior consigue la mejor ganancia en la proxima division.\n",
    "\n",
    "2.4. ¿Cómo se comporta la ganancia de información cuando compara atributos con cantidad de valores muy distintos? Por ejemplo, si tenemos un modelo que tiene los siguientes dos atributos: 1)\"tuvo_un_auto_previamente\" que tiene dos valores posibles (Si y No). 2) \"marca\", atributo con 40 valores distintos (sin orden), ¿es justo usar ganancia de información\n",
    "para elegir entre ellos? ¿Qué desventajas tiene? ¿Cómo se podría mitigar?\n",
    "\n",
    "No es justo, dado que no tiene en consideracion la cantidad de categorias que tiene cada feature, es por esto que los arboles, no se manejan bien cuando tienen muchas variables discretas con varias categorias, o muchas features continuas en el dataset. En estos casos, hay que hacer una ingenieria previa para disminuir este problema en el caso de ser posible (caso categorico)\n",
    "\n",
    "2.5 Si tiene atributos numéricos, ¿Qué cambios se ocurre que podria hacer para que su algoritmo funcione?\n",
    "\n",
    "En estos casos, se propone discretizar el dominio, para reducir la cantidad de categorias, dentro de la feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4-pSmO9Zy9L"
   },
   "source": [
    "# 3. Implementar un árbol de decisión\n",
    "\n",
    "A continuación se propone el **código incompleto** de un arbol de decisión junto a otro código referido a la visualización del mismo.\n",
    "\n",
    "**nota**: Si lo desea puede implementar su versión de cero del arbol de decisión\n",
    "\n",
    "3.1 Complete los siguientes dos métodos:\n",
    "\n",
    "* entropy(y): Una función que dado un vector como parámetro ( y ) con distintos valores,\n",
    "calcula su valor de entropía.\n",
    "\n",
    "* information_gain(c,y): Una función que toma una columna c y el vector y, y devuelve el la ganancia de información.\n",
    "\n",
    "\n",
    "3.2 Arme ejemplos de prueba chiquitos (como los vistos en la presentación del taller) para corroborrar que el árbol se arma bien. \n",
    "\n",
    "Por ejemplo, si una vez completado los métodos, corre el siguiente código\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Armamos dataset tennis \n",
    "X=np.array([[\"Sol\",\"Calor\",\"Alta\",\"Debil\"],\n",
    "[\"Sol\",\"Calor\",\"Alta\",\"Fuerte\"],\n",
    "[\"Nublado\",\"Calor\",\"Alta\",\"Debil\"],\n",
    "[\"Lluvia\",\"Templado\",\"Alta\",\"Debil\"],\n",
    "[\"Lluvia\",\"Frio\",\"Normal\",\"Debil\"],\n",
    "[\"Lluvia\",\"Frio\",\"Normal\",\"Fuerte\"],\n",
    "[\"Nublado\",\"Frio\",\"Normal\",\"Fuerte\"],\n",
    "[\"Sol\",\"Templado\",\"Alta\",\"Debil\"],\n",
    "[\"Sol\",\"Frio\",\"Normal\",\"Debil\"],\n",
    "[\"Lluvia\",\"Templado\",\"Normal\",\"Debil\"],\n",
    "[\"Sol\",\"Templado\",\"Normal\",\"Fuerte\"],\n",
    "[\"Nublado\",\"Templado\",\"Alta\",\"Fuerte\"],\n",
    "[\"Nublado\",\"Calor\",\"Normal\",\"Debil\"],\n",
    "[\"Lluvia\",\"Templado\",\"Alta\",\"Fuerte\"]])\n",
    "y=np.array('No No Si Si Si No Si No Si Si Si Si Si No'.split())\n",
    "attrs_names=np.array('Cielo Temperatura Humedad Viento'.split())\n",
    "\n",
    "\n",
    "# Creo un DecisionTree pasandole como parametro  de creacion el metodo information_gain\n",
    "dt= DecisionTree(information_gain)\n",
    "\n",
    "# Entrenamos\n",
    "dt.fit(X,y,attrs_names)\n",
    "\n",
    "# Pruebo una prediccion arbitraria\n",
    "prediccion_ejemplo= dt.predict(np.array([[\"Lluvia\",\"Frio\",\"Normal\",\"Fuerte\"]]))\n",
    "print('Predigo',prediccion_ejemplo)\n",
    "\n",
    "fig= dt.plot_graph()\n",
    "pylab.show()\n",
    "```\n",
    "\n",
    "Este código genera el siguiente árbol\n",
    "\n",
    "![alt text](https://docs.google.com/drawings/d/e/2PACX-1vRY3rHpayxjOp-8EvND35l6NELPXFLl-eaKLf1HuLHxKa1LswkijDyPNUkFqIrb7jbvmJZsFALWA2LG/pub?w=823&h=331)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZAuZsX05VpGq"
   },
   "outputs": [],
   "source": [
    "# Importo cosas\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pylab as pylab\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "# Defino la funcion log2 por comodidad\n",
    "def log2(v): return np.log(v)/np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "x0RqWdjzbrE_"
   },
   "outputs": [],
   "source": [
    "# Funciones a completar\n",
    "# entropy(y): Una función que dado un vector como parámetro ( y ) con distintos valores, calcula su valor de entropía.\n",
    "def entropy(y): \n",
    "  y = np.array(y)\n",
    "  cant_1 = np.unique(y, return_counts=True)[1][0]\n",
    "  cant_2 = np.unique(y, return_counts=True)[1][1]\n",
    "  p_clase0 = cant_1/len(y)\n",
    "  p_clase1 = cant_2/len(y)\n",
    "  return -(p_clase0 * (log2(p_clase0)) + p_clase1*log2(p_clase1))\n",
    "\n",
    "#information_gain(c,y): Una función que toma una columna c y el vector y, y devuelve la ganancia de información.\n",
    "def information_gain(c,y):\n",
    "  H = entropy(y)\n",
    "  proporcion_categoria0 = np.unique(c, return_counts=True)[1][0]/len(y)\n",
    "  proporcion_categoria1 = np.unique(c, return_counts=True)[1][1]/len(y)\n",
    "  H_categoria0 = entropy(y[c==0])\n",
    "  H_categoria1 = entropy(y[c==1])\n",
    "  print(\"H\",H)\n",
    "  print(\"h0, h1\", H_categoria0, H_categoria1)\n",
    "  print(\"proporciones cat 0 y 1\", proporcion_categoria0, proporcion_categoria1)\n",
    "  return H - (proporcion_categoria0*H_categoria0 + proporcion_categoria1*H_categoria1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_BzfWBRCIdR",
    "outputId": "55fa43b8-4a06-4c5a-913c-87ce97f8be2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H 0.8812908992306927\n",
      "h0, h1 0.9182958340544896 0.8112781244591328\n",
      "proporciones cat 0 y 1 0.6 0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0058021490143458365"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain(c, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "AIJbhS8BCMH4"
   },
   "outputs": [],
   "source": [
    "y = np.array([0, 0, 0 ,0, 1, 1, 1, 0, 0, 0])\n",
    "c = np.array([0,0,1,1,1,0,0,0,1,0])# Suponemos que solo tenemos 2 features X1 = 0 y X2 = 1, si habria mas features, hay que cambiar esto en la funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLQ20ca3MrUf",
    "outputId": "86e8ec65-d51e-4cbf-cd7b-60d2efcb86d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8812908992306927"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F6I1CZGVZRf"
   },
   "outputs": [],
   "source": [
    "# Clase de arbol de decision\n",
    "\t\t\n",
    "class DecisionTree():\n",
    "\t\"\"\" Inicializo el arbol, tengo que tomar la funcion que decida que \n",
    "\tatributo elegir \"\"\"\n",
    "\tdef __init__(self, attr_select_func):\n",
    "\t\t# guardo la funcion que usa para elegir el \"mejor\" atributo\n",
    "\t\tself.attr_select_func = attr_select_func\n",
    "\n",
    "\t\t# guarda la clase final si es una hoja\n",
    "\t\tself.predicted_class = None\n",
    "\t\t\n",
    "\n",
    "\t\tself.attr = \"\"\n",
    "\n",
    "\t\t# guardo los hijos\n",
    "\t\tself.sub_trees = {}\n",
    "\t\t\n",
    "\t\t# marco si ya ha sido llamado para fitearse o no\n",
    "\t\tself._fit_called = False\n",
    "\t\n",
    "\t\"\"\"Funcion para entrenar el arbol\"\"\"\n",
    "\tdef fit(self,X,y,attrs_names):\n",
    "\t\t# guardo el nombre de los atributos\n",
    "\t\tself.attrs_names = attrs_names\n",
    "\t\tself._fit_called=True\n",
    "\n",
    "\t\t# guardo el vector a fitear\n",
    "\t\tself._fit_y= y\n",
    "\t\t\n",
    "\t\t# Si no tengo mas atributos, guardo la clase con mayor cantidad\n",
    "\t\tif X.shape[1]==0:\n",
    "\t\t\tself.predicted_class = Counter(y).most_common(1)[0][0]\n",
    "\t\t\treturn None\n",
    "\t\t\t\n",
    "\t\t# Si \"y\" tiene una sola clase, termine seteo la clase para prediccion\n",
    "\t\tif all(y[0]==y):\n",
    "\t\t\tself.predicted_class= y[0]\n",
    "\t\t\treturn None\n",
    "\t\t\n",
    "\t\t# No termine tengo que seguir fiteando recursivamente\n",
    "\t\t\n",
    "\t\t# Obtengo el \"mejor\" atributo, usando la funcion\n",
    "\t\tindex_best_attr= np.argmax([self.attr_select_func(X[:,ci],y) for ci in range(X.shape[1])])\n",
    "\t\t\n",
    "\t\t# Armo las matrices para entrenamiento que van a recibir los \n",
    "\t\t# arboles hijos\n",
    "\t\tdifferent_values = sorted(set(X[:,index_best_attr]))\n",
    "\t\tX_without_column = np.delete(X,index_best_attr,axis=1)\n",
    "\t\tattrs_names_without_column = np.delete(attrs_names,index_best_attr,axis=0)\n",
    "\t\tself.attr = attrs_names[index_best_attr]\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tself.sub_trees={}\n",
    "\t\t\t\t\n",
    "\t\tfor dv in different_values:\n",
    "\t\t\t# Armo las submatrices para cada arbol hijo\n",
    "\t\t\tmask= X[:,index_best_attr]==dv \n",
    "\t\t\tnewX = X_without_column[mask,:]\n",
    "\t\t\tnewy = y[mask]\n",
    "\n",
    "\t\t\t# creo un arbol nuevo para cada hijo\n",
    "\t\t\t# notar que es una instancia de la misma clase!\n",
    "\t\t\tself.sub_trees[dv] = DecisionTree(self.attr_select_func)\n",
    "\t \n",
    "\t \t\t# llamo a fitear de este arbol nuevo que acabo de crear\n",
    "\t\t\tself.sub_trees[dv].fit(newX,newy,attrs_names_without_column)\n",
    "\t\n",
    "\t# funcion que sirve para predecir una nueva muestra\n",
    "\tdef predict(self,Xtest):\n",
    "\t\t# me fijo que el arbol este fiteado sino no tiene sentido predecir\n",
    "\t\tassert(self._fit_called),'El arbol aun no fue fitteado'\n",
    "\t\tres =[]\n",
    "\t\tdt=self\n",
    "\t\t\n",
    "\t\tfor irow in range(Xtest.shape[0]):\n",
    "\t\t\trow= Xtest[irow,:]\n",
    "\t\t\t# Si entreno con una muestra no total, esta es una forma de\n",
    "\t\t\t# mitigar el problema\n",
    "\t\t\ttry: res.append(self._predict(row))\n",
    "\t\t\texcept: res.append( Counter(self._fit_y).most_common(1)[0][0] )\n",
    "\t\t\t\n",
    "\t\treturn res\n",
    "\t\n",
    "\tdef _predict(self,row):\n",
    "\t\tif self.attr=='': return self.predicted_class\n",
    "\t\tindex_node_attr= np.where(self.attrs_names==self.attr)[0][0]\n",
    "\t\treturn self.sub_trees[row[index_node_attr]]._predict(np.delete(row,index_node_attr))\t\n",
    "\t\n",
    "\t### fin de clase arbol de decision ###\n",
    "\t### El siguiente codigo solo sirve para plotar el arbol, no hace falta\n",
    "\t### que lo mire (pero es mas q bienvenido en mirarlo, modificarlo, etc)\n",
    "\t\"\"\" Para plotear el arbol \t\"\"\"\n",
    "\tdef plot_graph(self):\n",
    "\t\t# Devuelve las posiciones para que se ploteee en forma de arbol\n",
    "\t\tdef _tree_layout(xcenter,ycenter,dt):\n",
    "\t\t\tres=[( id(dt),[xcenter,ycenter])]\n",
    "\t\t\ti=0\n",
    "\t\t\tfor k,sdt in dt.sub_trees.items():\n",
    "\t\t\t\tres+=_tree_layout((xcenter+ np.linspace(-.5/float(abs(ycenter-1)**2),0.5/float(abs(ycenter-1)**2),len(dt.sub_trees)))[i],ycenter-1,sdt)\n",
    "\t\t\t\ti+=1\n",
    "\t\t\treturn res\t\n",
    "\t\t\n",
    "\t\t# Bindea object id con label para plotting\n",
    "\t\tdef _create_dict_id_name(dt):\n",
    "\t\t\tres =[]\n",
    "\t\t\tif dt.attr==\"\": res.append((id(dt),dt.predicted_class))\n",
    "\t\t\telse: res.append((id(dt),dt.attr))\n",
    "\t\t\tfor k,v in dt.sub_trees.items(): \n",
    "\t\t\t\tres+=_create_dict_id_name(v)\n",
    "\t\t\treturn res\t\n",
    "\t\t\t\n",
    "\t\t# Obteng los edges\t\t\n",
    "\t\tdef _get_edges_to_plot(dt):\n",
    "\t\t\tedges=[]\n",
    "\t\t\tif dt.attr=='': return []\n",
    "\t\t\tfor k,v in dt.sub_trees.items(): \n",
    "\t\t\t\tedges.append((id(dt),id(v),k))\n",
    "\t\t\t\tif not v.attr=='': edges+= _get_edges_to_plot(v)\n",
    "\t\t\treturn edges\n",
    "\t\t\n",
    "\t\tdict_node_id_to_label = dict(_create_dict_id_name(self))\n",
    "\t\tedges=_get_edges_to_plot(self)\n",
    "\n",
    "\t\tpos =dict(_tree_layout(0,0,self))\n",
    "\n",
    "\t\tfig =pylab.figure(figsize=[ 14.7,   6. ])\n",
    "\t\t# Plot nodes\n",
    "\t\tfor k,coord in pos.items():\n",
    "\t\t\tpylab.scatter(coord[0],coord[1],s=500,alpha=0.3)\n",
    "\t\t\tpylab.text(coord[0],coord[1],dict_node_id_to_label[k],horizontalalignment='center', verticalalignment='center',fontsize=14)\n",
    "\t\t# Plot edges and labels\n",
    "\t\tfor src,dst,label in edges:\n",
    "\t\t\tsrc_point = pos[src]\t\n",
    "\t\t\tdst_point = pos[dst]\n",
    "\t\t\tpylab.plot([src_point[0],dst_point[0]],[src_point[1],dst_point[1]],c='black',alpha=0.5)\n",
    "\t\t\tpylab.text(src_point[0]+(dst_point[0]-src_point[0])/2.,src_point[1]+(dst_point[1]-src_point[1])/2.,label,\n",
    "\t\t\thorizontalalignment='center', verticalalignment='center',fontsize=12)\n",
    "\t\tpylab.xticks([])\n",
    "\t\tpylab.yticks([])\n",
    "\t\tfig.tight_layout()\n",
    "\t\treturn fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEtyJ_nmdlIK"
   },
   "source": [
    "# 4. Comparación con Scikit-learn\n",
    "\n",
    "5.1. Compare su implementación de árbol de decisión con la implementación que trae  ([scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). \n",
    "\n",
    "¿Cual anda mejor? ¿Por qué? Explore diferentes valores que ofrece scikit-learn. Pruebe usando cross-validation y diferentes medidas de performance vistas en la clase anterior. \n",
    "\n",
    "Para hacer diferentes pruebas puede usar el [este dataset](https://drive.google.com/file/d/1_vJKusl3AF_eOtEfKCzpY5_x42xtOwnu/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGYZ92iRbpPx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
