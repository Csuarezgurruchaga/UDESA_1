{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFsTQ9KQrb58"
   },
   "source": [
    "# Aprendizaje Supervisado II\n",
    "## Taller 1: métricas, validación cruzada y Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "*   1. Métricas\n",
    "*   2. Validación cruzada\n",
    "*   3. Naive Bayes\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Este notebook contiene los ejercicios del taller 1. Los mismos deberán ser realizados usando Python. Pueden hacerlo en colab como en sus propias computadoras.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En todos los casos que usemos variables que refieran a datos vamos a manejarnos con np.array salvo que se indique lo contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rk-rKVGBrdR9"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbwpll_ftnPy"
   },
   "source": [
    "#1. Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tykrFXZYkXyc"
   },
   "source": [
    "### 1.1 Programar las siguientes métricas como funciones de python con la siguiente aridad:\n",
    "\n",
    "\n",
    "```\n",
    "def metrica(y_true,y_pred):\n",
    "  ...\n",
    "  return score\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "1.   accuracy \n",
    "2.   precision\n",
    "3.   recall\n",
    "4.   F1-score\n",
    "5.   F$_{\\beta}$-score\n",
    "6.   Matriz de confusión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J2lXyZNFHnzU"
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "def accuracy(y_true, y_pred):\n",
    "\n",
    "  total = len(y_true)\n",
    "  t = sum(y_true == y_pred)\n",
    "  score = t/total\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BTMr2R43K0DJ"
   },
   "outputs": [],
   "source": [
    "# 2 tp / tp + fp\n",
    "def precision (y_true, y_pred):\n",
    "  tp = sum(np.logical_and((y_true == 1), (y_pred == 1)))\n",
    "  fp = sum(np.logical_and((y_true == 0), (y_pred == 1)))\n",
    "  score = tp / (tp + fp)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eLwysvg5McIR"
   },
   "outputs": [],
   "source": [
    "# 3 tp/ tp + fn\n",
    "def recall (y_true, y_pred):\n",
    "   tp = sum(np.logical_and((y_true == 1), (y_pred == 1)))\n",
    "   fn = sum(np.logical_and((y_true == 1), (y_pred == 0)))\n",
    "   score = tp / (tp + fn)\n",
    "   return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RmI95TWXPPve"
   },
   "outputs": [],
   "source": [
    "# 4 \n",
    "def f1_score2 (y_true, y_pred):\n",
    "  score = 2 * (precision(y_true, y_pred) * recall(y_true, y_pred)) / (precision(y_true, y_pred) + recall(y_true, y_pred))\n",
    "  return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UrdBU55OPPlj"
   },
   "outputs": [],
   "source": [
    "# 5\n",
    "def f_beta_score (y_true, y_pred, beta = 0.5):\n",
    "\n",
    "  score = (1 + beta**2) * (precision(y_true, y_pred) * recall(y_true, y_pred)) / ((beta**2 * precision(y_true, y_pred)) + recall(y_true, y_pred))\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "k-0CKNLCQa6T"
   },
   "outputs": [],
   "source": [
    "# 6\n",
    "def matriz_de_confusion(y_true, y_pred):\n",
    "   tp = sum(np.logical_and((y_true == 1), (y_pred == 1)))\n",
    "   tn = sum(np.logical_and((y_true == 0), (y_pred == 0)))\n",
    "   fn = sum(np.logical_and((y_true == 1), (y_pred == 0)))\n",
    "   fp = sum(np.logical_and((y_true == 0), (y_pred == 1)))\n",
    "\n",
    "   matriz = np.matrix([[tn, fp], [fn, tp]])\n",
    "\n",
    "   return matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqMwT2o5kZaa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXktX7G8KTIa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdrqFrEXoLEO"
   },
   "source": [
    "### 1.2 Crear diversos casos de test que prueben diferentes aspectos de cada metrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "K51_lyp8WcpD"
   },
   "outputs": [],
   "source": [
    "y_true = np.array([1, 0, 0, 1, 1, 0, 1, 0, 0])\n",
    "y_pred = np.array([1, 1, 0, 1, 0, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWdaJjPDoSyk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-oOsGhcoTY0"
   },
   "source": [
    "### 1.3 Comparar cada métrica con las provistas por scikit-learn usando los casos de test anteriores\n",
    "Referencia: https://scikit-learn.org/stable/modules/model_evaluation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kNDLEemMNk1f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ijYxW84oYAe",
    "outputId": "701bc3b4-079e-42b7-c718-b052915be2f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)\n",
    "accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75xBNE5nWh6z",
    "outputId": "bb3cede7-81aa-4ce4-cd38-a5e5e0d16a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n",
      "0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_true, y_pred))\n",
    "print(precision(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfwIJr4NWtuv",
    "outputId": "c242ab5d-fd3a-4180-92b7-3524a5642df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(y_true, y_pred))\n",
    "print(recall(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llbdwbDtWtsp",
    "outputId": "783708a0-74f2-4887-8f82-35c734966c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5454545454545454\n",
      "0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_true, y_pred))\n",
    "print(f1_score2(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdCkg-1QWtoB",
    "outputId": "6c4247e2-4bca-433d-a99b-c0af25e9f058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44959946595460615\n",
      "0.4495994659546061\n"
     ]
    }
   ],
   "source": [
    "print(fbeta_score(y_true, y_pred, beta = 0.35))\n",
    "print(f_beta_score(y_true, y_pred, 0.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k54jirwoWtjj",
    "outputId": "fb33fe58-5735-4593-b01a-6f50b29dc0fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [1 3]]\n",
      "[[1 4]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(matriz_de_confusion(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mobir3Q9tyGf"
   },
   "source": [
    "# 2. Validación cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4Yks6Nkt2TK"
   },
   "source": [
    "### 2.1 Programe validación cruzada de k-folds\n",
    "\n",
    "Una función que dado un entero $k$ y dos np.array $X$ e $y$. Construya una lista tuplas donde el primer elemento de cada tupla corresponde a los indices de las filas de $X$ e $Y$ que haran de fold de entrenamiento y el segundo elemento corresponde a los indices de las filas para el fold de test.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Notar que X.shape == ($N$,$f$) e y.shape == ($N$), con $N$ y $f$ dos enteros positivos.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "La partición de folds debe ser secuencial, es decir el primer fold debe contener los indices de las primeras $(N/k) \\times (k-1)$ para el train y los $N/k$ últimos para el fold de test y a si sucesivamente.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "\n",
    "```\n",
    "X = np.array( [ [91,92], [93,94], [95,96], [97,98], [99,100],[101,102], [103,104],[105,106] ])\n",
    "y = np.array( ['a','a','a','a','b','b','b','b'] )\n",
    "\n",
    "res = validación_cruzada_secuencial(X,y,4)\n",
    "res = [ \n",
    "        ( np.array([0,1,2,3,4,5])  , np.array([6,7]) ) ,\n",
    "        ( np.array([0,1,2,3,6,7])  , np.array([4,5]) ) ,\n",
    "        ( np.array([0,1,4,5,6,7])  , np.array([2,3]) ) ,\n",
    "        ( np.array([2,3,4,5,6,7])  , np.array([0,1]) ) ,\n",
    " ] \n",
    "```\n",
    "\n",
    "La aridad de la función tiene que ser la siguiente:\n",
    "```\n",
    "def validación_cruzada_secuencial(X,y,k):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NXQCRTV-uZJg"
   },
   "outputs": [],
   "source": [
    "def validacion_cruzada_secuencial(X, y, k):\n",
    "  N = y.shape[0]\n",
    "  dic_folds = {}\n",
    "  n_elementos_test = round(N/k)\n",
    "  indices = np.arange(0, N)\n",
    "  cont = k-1\n",
    "\n",
    "  for i in range(0, n_elementos_test*k, n_elementos_test):\n",
    "    test_index = indices[i:i+n_elementos_test]\n",
    "    \n",
    "    train_index = [x for x in indices if x not in test_index]    \n",
    "    dic_folds[cont] = (train_index, test_index)\n",
    "    cont = cont - 1\n",
    "\n",
    "  return dic_folds.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "niLF_lZGegCj"
   },
   "outputs": [],
   "source": [
    "# X = np.array( [ [91,92], [93,94], [95,96], [97,98], [99,100],[101,102], [103,104],[105,106], [107,108], [109, 110] ]) \n",
    "# y = np.array( ['a','a','a','a','b','b','b','b', 'b', 'b'] ) # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "JDI7KDjYAZwL"
   },
   "outputs": [],
   "source": [
    "X = np.array( [ [91,92], [93,94], [95,96], [97,98], [99,100],[101,102], [103,104],[105,106] ])\n",
    "y = np.array( ['a','a','a','a','b','b','b','b'] )# 8(0-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMDfRYJXef-Y",
    "outputId": "7ebe5c57-8a91-4615-bccd-71ea3973e505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([([2, 3, 4, 5, 6, 7], array([0, 1])), ([0, 1, 4, 5, 6, 7], array([2, 3])), ([0, 1, 2, 3, 6, 7], array([4, 5])), ([0, 1, 2, 3, 4, 5], array([6, 7]))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validacion_cruzada_secuencial(X, y, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPhIfU2G4XGb"
   },
   "source": [
    "### 2.2  ¿Cree que la implementación que ha hecho en el punto anterior tiene algun problema en el contexto de uso para clasificación? ¿Qué pasa con la distribución de la variable y entre los folds?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP1DsTxz4tjn"
   },
   "source": [
    "*Si, debido a que la particion es SECUENCIAL, podria ocurrir que si la muestra, presenta alguna tendencia en los ultimos datos, o en los primeros, ésta informacion no estara presente por igual en ambos set, con lo cual, el modelo no performara bien, al entrenarlo con datos sesgados.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCLpaUyuvlYZ"
   },
   "source": [
    "### 2.3 Modifique la implementación anterior de modo que la distribución de los valores de los fold de test sean similares. \n",
    "\n",
    "Es decir, en el ejemplo del punto 2.1, la implementación  ``` validación_cruzada_secuencial ``` genera los siguientes folds de test\n",
    "\n",
    "1. ['b','b']\n",
    "2. ['b','b']\n",
    "3. ['a','a']\n",
    "4. ['a','a']\n",
    "\n",
    "La nueva implementación debe tener la siguiente aridad\n",
    "\n",
    "``` def validacion_cruzada_estratificada(X,y,k): ``` \n",
    "\n",
    "Y se espera que genere los siguientes folds (no necesariamente estos pero uno similares)\n",
    "\n",
    "1. ['a' 'b']\n",
    "2. ['a' 'b']\n",
    "3. ['a' 'b']\n",
    "4. ['a' 'b']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Supongo que solo hay 2 categorias en test\n",
    "def validacion_cruzada_estratificada(X, y, k):\n",
    "    \n",
    "    cantidad_categorias = 2\n",
    "    proporcion_0 = sum(y==0)/len(y)\n",
    "    proporcion_1 = 1 - proporcion_0\n",
    "    \n",
    "    \n",
    "    cant_indices_clase0 = round(proporcion_0 * len(X))\n",
    "    cant_indices_clase1 = round(proporcion_1 * len(X))\n",
    "    \n",
    "    \n",
    "    # indices_categoria_0 = np.random.choice(np.where(y==0)[0], cant_indices_clase0, replace = False)\n",
    "    # indices_categoria_1 = np.random.choice(np.where(y==1)[0], cant_indices_clase1, replace = False)\n",
    "    \n",
    "    print(indices_categoria_0)\n",
    "    print(indices_categoria_1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2]\n",
      "[1 4 3 5]\n"
     ]
    }
   ],
   "source": [
    "validacion_cruzada_estratificada(X, y, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bA6GCuQO62gb"
   },
   "outputs": [],
   "source": [
    "def validacion_cruzada_estratificada(X, y, k):\n",
    "  N = y.shape[0]\n",
    "  dic_folds = {}\n",
    "  n_elementos_train = round((N/k)* (k-1))\n",
    "  cont = k-1\n",
    "  indices = range(0, N)\n",
    "\n",
    "  for i in range(0, k):\n",
    "    train_index = random.sample(indices, n_elementos_train)\n",
    "    test_index = [x for x in indices if x not in train_index]    \n",
    "    dic_folds[cont] = (train_index, test_index)\n",
    "    cont = cont - 1\n",
    "\n",
    "  return dic_folds.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFL6gG9s8NZ7",
    "outputId": "2fc33103-e59f-4849-e3c9-ccb50a89c70e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([([3, 2, 5, 1, 4, 7], [0, 6]), ([4, 5, 6, 1, 7, 2], [0, 3]), ([1, 0, 6, 5, 4, 2], [3, 7]), ([1, 0, 5, 2, 7, 4], [3, 6])])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validacion_cruzada_estratificada(X, y, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvsP6fDq7QSj"
   },
   "source": [
    "# 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx26Gun_Iq-D"
   },
   "source": [
    "### 3.1 Programar naive bayes para variables categoricas\n",
    "\n",
    "La aridad de la función debe ser la siguiente:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "def naive_bayes_categorico(X,y,new_sample):\n",
    "```\n",
    "\n",
    "Donde  ```X.shape == (N,f)```, ```y.shape == (f)```. $N$ representa la cantidad de muestras y $f$ la cantidad features o atributos\n",
    "\n",
    "Puede user de referencia las diapositivas de la clase o Capitulo 6 del libro: Machine Learning by Tom M. Mitchell [link](http://profsite.um.ac.ir/~monsefi/machine-learning/pdf/Machine-Learning-Tom-Mitchell.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9ibISw3YI02O"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Suponemos que tenemos 2 categorias a clasificar. CAMBIOS CAMBIOS\n",
    "\n",
    "def naive_bayes_categorico(X, y, new_sample):\n",
    "  N = X.shape[0] # cantidad de observaciones train\n",
    "  f = X.shape[1] # cantidad de features\n",
    "  dic_prob_clase_positiva = {}\n",
    "  dic_prob_clase_negativa = {}\n",
    "  cant_filas_new_sample = new_sample.shape[0] \n",
    "  new_sample_maped_0 = {}\n",
    "  new_sample_maped_1 = {}\n",
    "\n",
    "\n",
    "  clasificacion = np.zeros(len(new_sample), dtype = int)\n",
    "\n",
    "  #############################TRAIN#############################\n",
    "  prior_clase_0 = np.unique(y, return_counts = True)[1] [0] / N # casos favorables / casos totales\n",
    "  prior_clase_1 = np.unique(y, return_counts = True)[1] [1] / N\n",
    "\n",
    "\n",
    "  ##### DICT CLASE NEGATIVA\n",
    "  for feature in range(0, f):\n",
    "    (unicos_0, cantidad) = np.unique(X[y==0,feature], return_counts=True) # clase negativa \n",
    "    for u in range(0, len(unicos_0)):\n",
    "      dic_prob_clase_negativa[unicos_0[u]] = cantidad[u]/N\n",
    "  # print(\"diccionario_clase0\", dic_prob_clase_negativa)\n",
    "  \n",
    "\n",
    "  ##### DICT CLASE POSITIVA\n",
    "  for feature in range(0, f):\n",
    "    (unicos_1, cantidad) = np.unique(X[y==1,feature], return_counts=True) # clase positiva\n",
    "    for u in range(0, len(unicos_1)):\n",
    "       dic_prob_clase_positiva[unicos_1[u]] = cantidad[u]/N\n",
    "\n",
    "  # claves que no aparecieron en el train de la clase positiva\n",
    "  agregar_dic_positiva = [y for y in dic_prob_clase_negativa if y not in dic_prob_clase_positiva] \n",
    "  for agrego in agregar_dic_positiva:\n",
    "    dic_prob_clase_positiva[agrego]=1\n",
    "  \n",
    "  # claves que no aparecieron en el train de la clase negativa\n",
    "  agregar_dic_negativa = [t for t in dic_prob_clase_positiva if t not in dic_prob_clase_negativa] \n",
    "  for agrego in agregar_dic_negativa:\n",
    "    dic_prob_clase_negativa[agrego]=1\n",
    "\n",
    "  #############################TRABAJO_CON_NEW_SAMPLE###########################\n",
    "  new_sample_df = pd.DataFrame(new_sample)\n",
    "\n",
    "  for col in range(0, f):\n",
    "    new_sample_maped_0[col]=new_sample_df[col].map(dic_prob_clase_negativa)\n",
    "    new_sample_maped_1[col]=new_sample_df[col].map(dic_prob_clase_positiva)\n",
    "\n",
    "  new_sample_maped_0_df = pd.DataFrame(new_sample_maped_0)\n",
    "  new_sample_maped_1_df = pd.DataFrame(new_sample_maped_1)\n",
    "  \n",
    "  #############################PREDICCION#############################\n",
    "  for obs_new_sample in range(0, cant_filas_new_sample):\n",
    "    \n",
    "    if (new_sample_maped_1_df.product(axis = 1)[obs_new_sample] * prior_clase_1 < \n",
    "        new_sample_maped_0_df.product(axis = 1)[obs_new_sample] * prior_clase_0):\n",
    "      clasificacion[obs_new_sample] = 1\n",
    "\n",
    "  return clasificacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ext8eD_CKGrV"
   },
   "source": [
    "3.2 Cree diferentes datasets todas con variables categoricas para probar que su algoritmo funciona. Use las diferentes métricas y la validación cruzada que armó en los ejercicios anteriores o las versiones de scikit-learn para testear como funciona su algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_dLMTvnkUmJ3"
   },
   "outputs": [],
   "source": [
    "X = np.array([[\"u\",   \"u\",     \"u\",   \"u\",  \"c\",      \"b\",   \"b\",     \"a\",  \"c\",  \"c\"],\n",
    "              [\"uno\", \"uno\", \"uno\", \"uno\", \"cero\", \"cero\", \"cero\", \"cero\", \"uno\",\"uno\"]]).T\n",
    "y = np.array([   1,     1,      1,    1,     0,       0,      0,      0,     1,   1]).T\n",
    "\n",
    "# new_sample = np.array([[\"c\",     \"c\",    \"u\",    \"c\",   \"u\",   \"c\", \"u\",     \"u\",  \"b\",  \"b\"], \n",
    "#                        [\"cero\", \"cero\", \"uno\", \"cero\", \"cero\",\"uno\",\"cero\",\"uno\",\"uno\",\"uno\"]]).T\n",
    "\n",
    "new_sample = np.array([[\"u\",     \"u\",    \"b\",    \"c\",   \"a\", \"a\", \"c\",    \"c\", \"c\", \"c\"], \n",
    "                       [\"uno\", \"uno\", \"uno\", \"cero\", \"cero\",\"uno\",\"cero\",\"uno\",\"uno\",\"uno\"]]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7kPTOuNUq9mV"
   },
   "outputs": [],
   "source": [
    "y_pred = naive_bayes_categorico(X, y, new_sample)\n",
    "y_true = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3dJz5dBNOZC",
    "outputId": "e70fc830-dcd6-44cc-d510-a8873c3f7159"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3cW-zcENDkY",
    "outputId": "45c8042f-dcc2-4469-a161-10719e77de0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Bi-iK--NDiR",
    "outputId": "2a5a91a8-cc2e-4754-c954-f6dd5565caa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7oAvlPZMHnx",
    "outputId": "da20e738-fc29-4898-b748-6d3f5069ee2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score2(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zUcPPtPKevV"
   },
   "source": [
    "## 3.3 Programar naive bayes para que soporte variables numéricas. \n",
    "¿Cómo podria estimar $P(a_i|v_j)$? Piense en fitear diferentes distribuciones que le permitan luego inferir valores nuevos.\n",
    "\n",
    "Ayuda: Puede usera `scipy.stats.norm( mu , std ).pdf( new_val )` como funcion de densidad de probabilidad de una normal y evaluarla en new_val\n",
    "\n",
    "Pruebe sus resultados en el dataset iris\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oGuSG2GegpgJ"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from statistics import stdev\n",
    "\n",
    "def NaiveBayesGausseano(X, y, new_sample):\n",
    "  categorias = np.unique(y)\n",
    "  cant_features = X.shape[1]\n",
    "  medias_dic = {}\n",
    "  desvios_dic = {}\n",
    "  prior = {}\n",
    "  cont = 0\n",
    "  cont_2 = 0\n",
    "  podio_de_comparacion=np.zeros(len(categorias))\n",
    "  prob_dada_feature = {}\n",
    "  y_pred = []\n",
    "\n",
    "  for categoria in categorias:\n",
    "    X_por_categoria=pd.DataFrame(X[y==categoria])\n",
    "    for feature in range(0, cant_features):\n",
    "      medias_dic[cont] = X_por_categoria[feature].mean()\n",
    "      desvios_dic[cont] = stdev(X_por_categoria[feature])\n",
    "      cont = cont + 1\n",
    "  for cat in range(0, len(categorias)):\n",
    "    prior[cat] = np.unique(y, return_counts = True)[1][categoria]/len(y)\n",
    "\n",
    "  for observacion in new_sample:\n",
    "    for categoria in range(0, len(categorias)):\n",
    "      for feature in range(0, cant_features):\n",
    "        prob_dada_feature[feature]=norm(medias_dic[cont_2], desvios_dic[cont_2]).pdf(observacion[feature])\n",
    "        \n",
    "        cont_2 = cont_2 + 1\n",
    "      podio_de_comparacion[categoria] = np.prod(np.array(list(prob_dada_feature.values())))*prior[categoria]\n",
    "    y_pred.append(np.argmax(podio_de_comparacion))\n",
    "    cont_2 = 0\n",
    "  return np.array(y_pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2FaodPZNMYc9"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "PQf7X8XzaFmz"
   },
   "outputs": [],
   "source": [
    "a = random.sample(range(0,150), 75)\n",
    "new_sample = X[a,:]\n",
    "y_true = y[a]\n",
    "\n",
    "y_pred = NaiveBayesGausseano(X, y, new_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WesBei-XfU2Z",
    "outputId": "f0ff0e90-2a26-4500-a8b7-a2c496e10716"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n23JyU_1hdtc",
    "outputId": "84cffcbb-b595-4718-a765-b23ef9e48bda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsdquYvUhdrE",
    "outputId": "fcab6937-e926-4b22-c348-baf0cc426782"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJ0x_dkghdok",
    "outputId": "7e93c290-e0fa-4e78-f73d-bb7f1e0f60a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9743589743589743"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score2(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JCLpaUyuvlYZ"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
